{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "seed= 7\n",
    "np.seed= seed\n",
    "import gc\n",
    "import importlib\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set(style=\"whitegrid\", font_scale= 1.5)\n",
    "from IPython.display import display\n",
    "\n",
    "# modelling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Neural Networks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis, IMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.read_csv( '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    50000.000000\n",
       "mean       231.156940\n",
       "std        171.343997\n",
       "min          4.000000\n",
       "25%        126.000000\n",
       "50%        173.000000\n",
       "75%        280.000000\n",
       "max       2470.000000\n",
       "Name: token_length, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['token_length']= data['review'].apply(lambda x: len(x.split()) )\n",
    "plt.hist( data['token_length'], bins= 50 )\n",
    "plt.xlabel('')\n",
    "plt.show()\n",
    "\n",
    "data['token_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    0.5\n",
       "positive    0.5\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts(normalize= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (40000,)\n",
      "X_test shape:  (10000,)\n",
      "\n",
      "distribution of sentiments in the train data: \n",
      " positive    0.5\n",
      "negative    0.5\n",
      "Name: sentiment, dtype: float64\n",
      "\n",
      "distribution of sentiments in the test data: \n",
      " positive    0.5\n",
      "negative    0.5\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test= train_test_split (data, test_size=0.2, random_state= seed, \n",
    "                                        shuffle= True, stratify= data['sentiment'])\n",
    "\n",
    "X_train= data_train['review']\n",
    "y_train= data_train['sentiment']\n",
    "X_test= data_test['review']\n",
    "y_test= data_test['sentiment']\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print()\n",
    "print( 'distribution of sentiments in the train data: \\n', \n",
    "              data_train['sentiment'].value_counts(normalize= True) )\n",
    "print()\n",
    "print( 'distribution of sentiments in the test data: \\n',  \n",
    "              data_test['sentiment'].value_counts(normalize= True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37667    There were very few good moments in this film....\n",
       "48751    Soylent Green is a classic. I have been waitin...\n",
       "Name: review, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "25900    It is sad what they are letting into film fest...\n",
       "36505    I find this movie very enjoyable. The plot is ...\n",
       "Name: review, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "37667    negative\n",
       "48751    positive\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train.head(2))\n",
    "display(X_test.head(2))\n",
    "display(y_train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "X_train= X_train.apply(lambda row: clean_doc(row) )\n",
    "X_test= X_test.apply(lambda row: clean_doc(row) )\n",
    "\n",
    "y_train= y_train.apply( lambda x: 1 if x=='positive' else 0)\n",
    "y_test= y_test.apply( lambda x: 1 if x=='positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37667    [There, good, moments, film, Only, couple, cha...\n",
       "48751    [Soylent, Green, classic, waiting, someone, re...\n",
       "Name: review, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'good',\n",
       " 'moments',\n",
       " 'film',\n",
       " 'Only',\n",
       " 'couple',\n",
       " 'characters',\n",
       " 'fleshed',\n",
       " 'well',\n",
       " 'There',\n",
       " 'plot',\n",
       " 'holes',\n",
       " 'big',\n",
       " 'enough',\n",
       " 'drive',\n",
       " 'truck',\n",
       " 'The',\n",
       " 'pace',\n",
       " 'creeped',\n",
       " 'along',\n",
       " 'like',\n",
       " 'old',\n",
       " 'man',\n",
       " 'There',\n",
       " 'many',\n",
       " 'moments',\n",
       " 'film',\n",
       " 'never',\n",
       " 'came',\n",
       " 'back',\n",
       " 'like',\n",
       " 'Coco',\n",
       " 'stripping',\n",
       " 'What',\n",
       " 'happened',\n",
       " 'How',\n",
       " 'Garcis',\n",
       " 'sister',\n",
       " 'Is',\n",
       " 'better',\n",
       " 'What',\n",
       " 'Leroy',\n",
       " 'We',\n",
       " 'learned',\n",
       " 'absolutely',\n",
       " 'nothing',\n",
       " 'What',\n",
       " 'electronic',\n",
       " 'piano',\n",
       " 'guy',\n",
       " 'How',\n",
       " 'rich',\n",
       " 'girl',\n",
       " 'got',\n",
       " 'abortion',\n",
       " 'What',\n",
       " 'happened',\n",
       " 'That',\n",
       " 'interesting',\n",
       " 'subplotbr',\n",
       " 'br',\n",
       " 'Overall',\n",
       " 'good',\n",
       " 'movie',\n",
       " 'recommend',\n",
       " 'another',\n",
       " 'musical',\n",
       " 'film',\n",
       " 'LETS',\n",
       " 'DO',\n",
       " 'THE',\n",
       " 'TIME',\n",
       " 'WARP',\n",
       " 'AGAIN']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we tokenize text data?**\n",
    "\n",
    "Most NLP applications need an embedding layer. An embedding layer is usually put in the first layer, and its purpose is to learn meaningful representations of words. There are a variety of neural network architechture to learn these embeggings. The input of an embedding layer is a fixed number of words, and the output is their embeddings. The mechanism is explained by me ine one page. \n",
    "\n",
    "However, the layer cannot accept words as they are. In reality, we need to one-hot encode the words first. However, an equivalent way of one-hot encoding is integer encoding of words. Every word in the vocabulary gets a unique integer assigned to it. This process is sometimes called tokenization. The following function does so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    \"\"\"\n",
    "    returns an object for to tokenize words in each document of our dataset.\n",
    "    lines is a list of lists. Each sublist in lines (representing one document) \n",
    "    is a list of words.\n",
    "    \n",
    "    Ex. \n",
    "    \n",
    "    integer_encoded= create_tokenizer(lines).tests_to_sequences( texts )\n",
    "    texts is a dataframe or series or list of elments where elements are lists of words\n",
    "    \n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find out the maximum length of our documents. If you remember, our documents are now in form of a list of words. We need to know the maximum length because later on we'll have to pad shorter documents by as much as this maximum size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1480"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the maximum document length\n",
    "def max_length(lists):\n",
    "    \"\"\"\n",
    "    lists is a list of lists\n",
    "    It returns the maximum length of its elements\n",
    "    \"\"\"\n",
    "    return max([len( l ) for l in lists])\n",
    "\n",
    "max_length( X_train.tolist() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen= length, padding= 'post')\n",
    "    return padded\n",
    "\n",
    "tokenizer= create_tokenizer( X_train.tolist() )\n",
    "encoded_text= encode_text( tokenizer, X_train.tolist(),  max_length( X_train.tolist() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "1480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9065, 1339,  270, ...,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(encoded_text))\n",
    "print( len( encoded_text[0] ) )\n",
    "encoded_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "Here we fit a multi-channel CNN to predict sentiments from imdb reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1480)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1480)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1480)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1480, 100)    15462100    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1480, 100)    15462100    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1480, 100)    15462100    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 1477, 32)     12832       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1475, 32)     19232       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1473, 32)     25632       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1477, 32)     0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1475, 32)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1473, 32)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 738, 32)      0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 737, 32)      0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 736, 32)      0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 23616)        0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 23584)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 23552)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 70752)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           707530      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            11          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 47,151,537\n",
      "Trainable params: 47,151,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 40000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 492s 12ms/sample - loss: 0.6946 - accuracy: 0.5038\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 499s 12ms/sample - loss: 0.5447 - accuracy: 0.7768\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 495s 12ms/sample - loss: 0.4072 - accuracy: 0.9305\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 488s 12ms/sample - loss: 0.3569 - accuracy: 0.9558\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 507s 13ms/sample - loss: 0.3332 - accuracy: 0.9673\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 529s 13ms/sample - loss: 0.3165 - accuracy: 0.9719\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 514s 13ms/sample - loss: 0.3048 - accuracy: 0.9746\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 511s 13ms/sample - loss: 0.2935 - accuracy: 0.9769\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 502s 13ms/sample - loss: 0.2842 - accuracy: 0.9779\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 518s 13ms/sample - loss: 0.2763 - accuracy: 0.9783\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs= outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes= True, to_file= 'multichannel.png')\n",
    "    return model\n",
    "########################################3\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "length= max_length( X_train.tolist() )\n",
    "\n",
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([encoded_text,encoded_text,encoded_text], y_train.values, epochs=10, batch_size=1024)\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1208"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length( X_test.tolist() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoded_test_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a0577b1c9d49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_test_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'encoded_test_text' is not defined"
     ]
    }
   ],
   "source": [
    "del(encoded_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text_test= encode_text( tokenizer, X_test.tolist(),  max_length( X_train.tolist() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 22s 2ms/sample - loss: 0.4850 - accuracy: 0.8830\n",
      "Test Accuracy: 88.300002\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('model.h5')\n",
    "# evaluate model on training dataset\n",
    "#loss, acc = model.evaluate([encoded_text,encoded_text,encoded_text], y_train.values, verbose=1)\n",
    "#print('Train Accuracy: %f' % (acc*100))\n",
    " \n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([encoded_text_test,encoded_text_test,encoded_text_test], y_test.values, verbose=1)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
