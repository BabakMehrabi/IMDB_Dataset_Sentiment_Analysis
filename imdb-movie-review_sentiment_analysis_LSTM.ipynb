{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, we would like to solve imdb review sentiment classification task via an RNN approach. More specifically by an LSTM\n",
    "\n",
    "# import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "seed= 7\n",
    "np.seed= seed\n",
    "import gc\n",
    "import importlib\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set(style=\"whitegrid\", font_scale= 1.5)\n",
    "from IPython.display import display\n",
    "\n",
    "# modelling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# text preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "# Neural Networks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "\"\"\"from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\"\"\"\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.read_csv( '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\n"
     ]
    }
   ],
   "source": [
    "print( data['review'].iloc[3] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    0.5\n",
       "negative    0.5\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts(normalize= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (40000,)\n",
      "X_test shape:  (10000,)\n",
      "\n",
      "distribution of sentiments in the train data: \n",
      " positive    0.5\n",
      "negative    0.5\n",
      "Name: sentiment, dtype: float64\n",
      "\n",
      "distribution of sentiments in the test data: \n",
      " positive    0.5\n",
      "negative    0.5\n",
      "Name: sentiment, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train, data_test= train_test_split (data, test_size=0.2, random_state= seed, \n",
    "                                        shuffle= True, stratify= data['sentiment'])\n",
    "\n",
    "X_train= data_train['review']; X_train.index= range(len(X_train))\n",
    "y_train= data_train['sentiment']; y_train.index= range(len(y_train))\n",
    "X_test= data_test['review']; X_test.index= range(len(X_test))\n",
    "y_test= data_test['sentiment']; y_test.index= range(len(y_test))\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print()\n",
    "print( 'distribution of sentiments in the train data: \\n', \n",
    "              data_train['sentiment'].value_counts(normalize= True) )\n",
    "print()\n",
    "print( 'distribution of sentiments in the test data: \\n',  \n",
    "              data_test['sentiment'].value_counts(normalize= True) )\n",
    "\n",
    "del(data_train, data_test); gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "X_train= X_train.apply(lambda row: clean_doc(row) )\n",
    "X_test= X_test.apply(lambda row: clean_doc(row) )\n",
    "\n",
    "y_train= y_train.apply( lambda x: 1 if x=='positive' else 0)\n",
    "y_test= y_test.apply( lambda x: 1 if x=='positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [There, good, moments, film, Only, couple, cha...\n",
       "1    [Soylent, Green, classic, waiting, someone, re...\n",
       "Name: review, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we tokenize text data?**\n",
    "\n",
    "Most NLP applications need an embedding layer. An embedding layer is usually put in the first layer, and its purpose is to learn meaningful representations of words. There are a variety of neural network architechture to learn these embeggings. The input of an embedding layer is a fixed-sized window of words, and the output is their embeddings. The mechanism is explained by me in one sheet of paper. \n",
    "\n",
    "However, the layer cannot accept words as they are. In reality, we need to one-hot encode the words first. However, an equivalent way of one-hot encoding is integer encoding of words. Every word in the vocabulary gets a unique integer assigned to it. This process is sometimes called tokenization. The following function implements it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    \"\"\"\n",
    "    returns an object for to tokenize words in each document of our dataset.\n",
    "    lines is a list of lists. Each sublist in lines (representing one document) \n",
    "    is a list of words.\n",
    "    \n",
    "    Ex. \n",
    "    \n",
    "    integer_encoded= create_tokenizer(lines).tests_to_sequences( texts )\n",
    "    texts is a dataframe or series or list of elments where elements are lists of words\n",
    "    \n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find out the maximum length of our documents. If you remember, our documents are now in form of a list of words. We need to know the maximum length because later on we'll have to pad shorter documents with zeros and by as much as this maximum size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1480"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the maximum document length\n",
    "def max_length(lists):\n",
    "    \"\"\"\n",
    "    lists is a list of lists\n",
    "    It returns the maximum length of its elements\n",
    "    \"\"\"\n",
    "    return max([len( l ) for l in lists])\n",
    "\n",
    "max( max_length( X_train.tolist() ), max_length( X_test.tolist() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        tokenizer is a tokenizer object, like the one we fit before\n",
    "        lines is a list of elements where each element is a list of words, and it stands for one document\n",
    "        length is the maximum length of documents in the train and test data\n",
    "    \n",
    "    It integer encodes the texts, pad them with zeros by as much as length and returns a list of lists.\n",
    "    \"\"\"    \n",
    "    \n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = sequence.pad_sequences(encoded, maxlen= length )\n",
    "    return padded\n",
    "\n",
    "tokenizer= create_tokenizer( X_train.tolist() )\n",
    "length= max( max_length( X_train.tolist() ), max_length( X_test.tolist() ) )\n",
    "encoded_train= encode_text( tokenizer, X_train.tolist(), length )\n",
    "encoded_test= encode_text( tokenizer, X_test.tolist(), length )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1480, 100)         15462100  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,542,601\n",
      "Trainable params: 15,542,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.6212 - accuracy: 0.6777\n",
      "Epoch 2/3\n",
      "40000/40000 [==============================] - 68s 2ms/step - loss: 0.4892 - accuracy: 0.8206\n",
      "Epoch 3/3\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.2507 - accuracy: 0.8996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc6d319c390>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector_length= 100\n",
    "# we add 1 because of 0 from padding\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# length is the maximum review length \n",
    "length= max( max_length( X_train.tolist() ), max_length( X_test.tolist() ) )\n",
    "\n",
    "\n",
    "model= Sequential()\n",
    "model.add( Embedding(vocab_size, embedding_vector_length, input_length= length ))\n",
    "model.add( LSTM(100) )\n",
    "model.add( Dense(1, activation= 'sigmoid') )\n",
    "model.compile( loss= 'binary_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(encoded_train, y_train, epochs= 3, batch_size= 1024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 67s 7ms/step\n",
      "Accuracy: 87.45 %\n"
     ]
    }
   ],
   "source": [
    "scores= model.evaluate( encoded_test, y_test)\n",
    "print('Accuracy: %.2f %%' % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
